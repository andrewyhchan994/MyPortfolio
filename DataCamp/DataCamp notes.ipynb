{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89e53c94",
   "metadata": {},
   "source": [
    "Seaborn\n",
    "Create a scatter plot of GDP (gdp) vs. number of phones per 1000 people (phones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b8c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Matplotlib and Seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create scatter plot with GDP on the x-axis and number of phones on the y-axis\n",
    "sns.scatterplot(x=gdp, y = phones)\n",
    "\n",
    "#show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aae4a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create count plot with region on the y-axis\n",
    "sns.countplot(y=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10063321",
   "metadata": {},
   "source": [
    "tidy data: each observation has its own row\n",
    "each variable has its own column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad96b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#countplot with a dataframe df\n",
    "\n",
    "# Create a count plot with \"Spiders\" on the x-axis\n",
    "sns.countplot(x=\"Spiders\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5fa072",
   "metadata": {},
   "source": [
    "Create a scatter plot with \"absences\" on the x-axis and final grade (\"G3\") on the y-axis using the DataFrame student_data. Color the plot points based on \"location\" (urban vs. rural)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678109bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the legend order in the scatter plot\n",
    "sns.scatterplot(x=\"absences\", y=\"G3\", \n",
    "                data=student_data, \n",
    "                hue=\"location\", hue_order=[\"Rural\", \"Urban\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70549543",
   "metadata": {},
   "source": [
    "Using palettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9a76cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping subgroup values to colors\n",
    "palette_colors = {\"Rural\": \"green\", \"Urban\": \"blue\"}\n",
    "\n",
    "# Create a count plot of school with location subgroups\n",
    "sns.countplot(x=\"school\", data=student_data, hue=\"location\", palette=palette_colors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b617d69",
   "metadata": {},
   "source": [
    "Relplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69568eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to make subplots based on study time\n",
    "sns.relplot(x=\"absences\", y=\"G3\", \n",
    "            data=student_data,\n",
    "            kind=\"scatter\")\n",
    "\n",
    "#create multiple scatterplot with col=\"var_name\"\n",
    "sns.relplot(x=\"absences\", y=\"G3\", \n",
    "            data=student_data,\n",
    "            kind=\"scatter\", \n",
    "            col=\"study_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bfb952",
   "metadata": {},
   "source": [
    "relplot with col_order and row_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675cd018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust further to add subplots based on family support\n",
    "sns.relplot(x=\"G1\", y=\"G3\", \n",
    "            data=student_data,\n",
    "            kind=\"scatter\", \n",
    "            col=\"schoolsup\",\n",
    "            col_order=[\"yes\", \"no\"],\n",
    "            row=\"famsup\",\n",
    "            row_order=[\"yes\", \"no\"]\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c86062",
   "metadata": {},
   "source": [
    "size, style,and point, and transparency can allow further customization of our plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba5eba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vary style and color by the origin param (origin is categorical)\n",
    "\n",
    "sns.relplot(x=\"acceleration\", y=\"mpg\", data=mpg, style=\"origin\", hue=\"origin\", kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb812ab",
   "metadata": {},
   "source": [
    "line plot showing distribution (using standard deviation) instead of confidence interval around mean with ci=\"sd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7562b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the shaded area show the standard deviation\n",
    "sns.relplot(x=\"model_year\", y=\"mpg\",\n",
    "            data=mpg, kind=\"line\", ci=\"sd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e77174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add markers and make each line have the same style\n",
    "sns.relplot(x=\"model_year\", y=\"horsepower\", \n",
    "            data=mpg, kind=\"line\", \n",
    "            ci=None, style=\"origin\", \n",
    "            hue=\"origin\", dashes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffec203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add markers and make each line have the same style\n",
    "sns.relplot(x=\"model_year\", y=\"horsepower\", \n",
    "            data=mpg, kind=\"line\", \n",
    "            ci=None, style=\"origin\", \n",
    "            hue=\"origin\", markers=True,dashes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6a3ff3",
   "metadata": {},
   "source": [
    "count plot with catplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7f22cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the orientation of the plot\n",
    "sns.catplot(x=\"Internet usage\", data=survey_data,\n",
    "            kind=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73907823",
   "metadata": {},
   "source": [
    "barplot with catplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5727029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot of interest in math, separated by gender\n",
    "sns.catplot(x=\"Gender\", y=\"Interested in Math\", data=survey_data, kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d7d663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categories from lowest to highest\n",
    "category_order = [\"<2 hours\", \n",
    "                  \"2 to 5 hours\", \n",
    "                  \"5 to 10 hours\", \n",
    "                  \">10 hours\"]\n",
    "\n",
    "# Turn off the confidence intervals\n",
    "sns.catplot(x=\"study_time\", y=\"G3\",\n",
    "            data=student_data,\n",
    "            kind=\"bar\",\n",
    "            order=category_order, ci=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8141d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove outliers with sym=\"\", color boxplots based on location(urban rural)\n",
    "sns.catplot(x=\"internet\", y=\"G3\", data=student_data, kind=\"box\", hue=\"location\", sym=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d851e0f4",
   "metadata": {},
   "source": [
    "Point plot using catplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1a3efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove lines between category, and add a cap on top and bottom with capsize=0.2\n",
    "sns.catplot(x=\"famrel\", y=\"absences\",\n",
    "\t\t\tdata=student_data,\n",
    "            kind=\"point\",\n",
    "            capsize=0.2, join=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c83c94",
   "metadata": {},
   "source": [
    "set_style(), \"whitegrid\", set_palette() \"Purples\", \"RdBu\", set_context() \"Paper\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29621c97",
   "metadata": {},
   "source": [
    "plot titles and axis labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d1a4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#titles for relplot and catplot()\n",
    "# Create scatter plot\n",
    "g = sns.relplot(x=\"weight\", \n",
    "                y=\"horsepower\", \n",
    "                data=mpg,\n",
    "                kind=\"scatter\")\n",
    "\n",
    "# Add a title \"Car Weight vs. Horsepower\"\n",
    "g.fig.suptitle(\"Car Weight vs. Horsepower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a4177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add x-axis and y-axis labels\n",
    "\n",
    "g.set(xlabel=\"Car Model Year\",\n",
    "    ylabel=\"Average MPG\")\n",
    "\n",
    "\n",
    "# Rotate x-tick labels\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7b98cb",
   "metadata": {},
   "source": [
    "facetgrid -> sns.fig.suptitle -> overall\n",
    "axesgrid -> sns.set_title(\" \") -> subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb0a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "# Adjust to add subplots per gender\n",
    "g = sns.catplot(x=\"Village - town\", y=\"Likes Techno\", \n",
    "                data=survey_data, kind=\"bar\",\n",
    "                col=\"Gender\")\n",
    "\n",
    "# Add title and axis labels\n",
    "g.fig.suptitle(\"Percentage of Young People Who Like Techno\", y=1.02)\n",
    "g.set(xlabel=\"Location of Residence\", \n",
    "       ylabel=\"% Who Like Techno\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e98d0a7",
   "metadata": {},
   "source": [
    "Importing data in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88e197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file: file\n",
    "file = open(\"moby_dick.txt\", \"r\")\n",
    "\n",
    "# Print it\n",
    "print(file.read())\n",
    "\n",
    "# Check whether file is closed\n",
    "print(file.closed)\n",
    "\n",
    "# Close file\n",
    "file.close()\n",
    "\n",
    "# Check whether file is closed\n",
    "print(file.closed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b61a4e",
   "metadata": {},
   "source": [
    "Using Context manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7973f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read & print the first 3 lines\n",
    "with open('moby_dick.txt') as file:\n",
    "    print(file.readline())\n",
    "    print(file.readline())\n",
    "    print(file.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5347f28a",
   "metadata": {},
   "source": [
    "Import flat files as numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25c793f",
   "metadata": {},
   "source": [
    "Complete the arguments of np.loadtxt(): the file you're importing is tab-delimited, you want to skip the first row and you only want to import the first and third columns.\n",
    "Complete the argument of the print() call in order to print the entire array that you just imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfbe5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the filename: file\n",
    "file = 'digits_header.txt'\n",
    "\n",
    "# Load the data: data\n",
    "data = np.loadtxt(file, delimiter=\"\\t\", skiprows=1, usecols=[0, 2])\n",
    "\n",
    "# Print data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52d52c1",
   "metadata": {},
   "source": [
    "Complete the second call to np.loadtxt(). The file you're importing is tab-delimited, the datatype is float, and you want to skip the first row.\n",
    "Print the 10th element of data_float by completing the print() command. Be guided by the previous print() call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8357842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign filename: file\n",
    "file = 'seaslug.txt'\n",
    "# Import data as floats and skip the first row: data_float\n",
    "data_float = np.loadtxt(file, delimiter=\"\\t\", dtype=\"float\", skiprows=1)\n",
    "\n",
    "# Print the 10th element of data_float\n",
    "print(data_float[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63169829",
   "metadata": {},
   "source": [
    "pandas import flat files and converting dataframe into numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5471272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the filename: file\n",
    "file = 'digits.csv'\n",
    "\n",
    "# Read the first 5 rows of the file into a DataFrame: data\n",
    "data = pd.read_csv(file, nrows = 5, header = None)\n",
    "\n",
    "# Build a numpy array from the DataFrame: data_array\n",
    "data_array = np.array(data)\n",
    "\n",
    "# Print the datatype of data_array to the shell\n",
    "print(type(data_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed94c813",
   "metadata": {},
   "source": [
    "using sep = \\t and comment = '#'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7f57fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign filename: file\n",
    "file = 'titanic_corrupt.txt'\n",
    "\n",
    "# Import file: data\n",
    "data = pd.read_csv(file, sep=\"\\t\", comment=\"#\", na_values=[\"Nothing\"])\n",
    "\n",
    "# Print the head of the DataFrame\n",
    "print(data.head())\n",
    "\n",
    "# Plot 'Age' variable in a histogram\n",
    "pd.DataFrame.hist(data[['Age']])\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925d539a",
   "metadata": {},
   "source": [
    "Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebcad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pickle file and load data: d\n",
    "with open('data.pkl', mode='rb') as file:\n",
    "    d = pickle.load(file)\n",
    "\n",
    "# Print d\n",
    "print(d)\n",
    "\n",
    "# Print datatype of d\n",
    "print(type(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c810661",
   "metadata": {},
   "source": [
    "Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721999b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign spreadsheet filename: file\n",
    "file = \"battledeath.xlsx\"\n",
    "\n",
    "# Load spreadsheet: xls\n",
    "xls = pd.ExcelFile(file)\n",
    "\n",
    "# Print sheet names\n",
    "print(xls.sheet_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892e8d6",
   "metadata": {},
   "source": [
    "Using usecols and skiprows\n",
    "Parse the first sheet by index. In doing so, skip the first row of data and name the columns 'Country' and 'AAM due to War (2002)' using the argument names. The values passed to skiprows and names all need to be of type list.\n",
    "Parse the second sheet by index. In doing so, parse only the first column with the usecols parameter, skip the first row and rename the column 'Country'. The argument passed to usecols also needs to be of type list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241dc3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the first sheet and rename the columns: df1\n",
    "df1 = xls.parse(0, skiprows=[0], names=[\"Country\", \"AAM due to War (2002)\"])\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(df1.head())\n",
    "\n",
    "# Parse the first column of the second sheet and rename the column: df2\n",
    "df2 = xls.parse(1, usecols=[0], skiprows=[0], names=[\"Country\"])\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3c5297",
   "metadata": {},
   "source": [
    "SAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c7bd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sas7bdat package\n",
    "from sas7bdat import SAS7BDAT\n",
    "\n",
    "# Save file to a DataFrame: df_sas\n",
    "with SAS7BDAT('sales.sas7bdat') as file:\n",
    "    df_sas = file.to_data_frame()\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df_sas.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6fe88e",
   "metadata": {},
   "source": [
    "STATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Stata file into a pandas DataFrame: df\n",
    "df = pd.read_stata('disarea.dta')\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d105079",
   "metadata": {},
   "source": [
    "HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c630d1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the HDF5 group: group\n",
    "group = data['strain']\n",
    "\n",
    "# Check out keys of group\n",
    "for key in group.keys():\n",
    "    print(key)\n",
    "\n",
    "# Set variable equal to time series data: strain\n",
    "strain = data['strain']['Strain'].value\n",
    "\n",
    "# Set number of time points to sample: num_samples\n",
    "num_samples = 10000\n",
    "\n",
    "# Set time vector\n",
    "time = np.arange(0, 1, 1/num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c5e50a",
   "metadata": {},
   "source": [
    "MATLAB\n",
    "Use the method .keys() on the dictionary mat to print the keys. Most of these keys (in fact the ones that do NOT begin and end with '__') are variables from the corresponding MATLAB environment.\n",
    "Print the type of the value corresponding to the key 'CYratioCyt' in mat. Recall that mat['CYratioCyt'] accesses the value.\n",
    "Print the shape of the value corresponding to the key 'CYratioCyt' using the numpy function shape().\n",
    "Execute the entire script to see some oscillatory gene expression data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the keys of the MATLAB dictionary\n",
    "print(mat.keys())\n",
    "\n",
    "# Print the type of the value corresponding to the key 'CYratioCyt'\n",
    "print(type(mat['CYratioCyt']))\n",
    "\n",
    "# Print the shape of the value corresponding to the key 'CYratioCyt'\n",
    "print(np.shape(mat['CYratioCyt']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193eedfb",
   "metadata": {},
   "source": [
    "RDMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2e319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary module\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3171418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary module\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Save the table names to a list: table_names\n",
    "table_names = engine.table_names()\n",
    "\n",
    "# Print the table names to the shell\n",
    "print(table_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefc8031",
   "metadata": {},
   "source": [
    "Querying SQL in python and also using pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de066d48",
   "metadata": {},
   "source": [
    "Open the engine connection as con using the method connect() on the engine.\n",
    "Execute the query that selects ALL columns from the Album table. Store the results in rs.\n",
    "Store all of your query results in the DataFrame df by applying the fetchall() method to the results rs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bc3ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine connection: con\n",
    "con = engine.connect()\n",
    "\n",
    "# Perform query: rs\n",
    "rs = con.execute('SELECT * FROM Album')\n",
    "\n",
    "# Save results of the query to DataFrame: df\n",
    "df = pd.DataFrame(rs.fetchall())\n",
    "\n",
    "# Close connection\n",
    "con.close()\n",
    "\n",
    "# Print head of DataFrame df\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584841c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### fetching only 3 of records with fetchmany()\n",
    "### Also using with.... as similar to opening file\n",
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT LastName, Title FROM Employee\")\n",
    "    df = pd.DataFrame(rs.fetchmany(3)) #fetch only 3 of records\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "# Print the length of the DataFrame df\n",
    "print(len(df))\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5ba1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## example 2\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute('SELECT * FROM Employee WHERE EmployeeId >= 6')\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14e2b7f",
   "metadata": {},
   "source": [
    "Using the function create_engine(), create an engine for the SQLite database Chinook.sqlite and assign it to the variable engine.\n",
    "In the context manager, execute the query that selects all records from the Employee table and orders them in increasing order by the column BirthDate. Assign the result to rs.\n",
    "In a call to pd.DataFrame(), apply the method fetchall() to rs in order to fetch all records in rs. Store them in the DataFrame df.\n",
    "Set the DataFrame's column names to the corresponding names of the table columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66a67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine in context manager\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT * FROM Employee ORDER BY BirthDate ASC\")\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "\n",
    "    # Set the DataFrame's column names\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173400a4",
   "metadata": {},
   "source": [
    "pandas   pd.read_sql_query() compared to using 'context manager' with .... as...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef355746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Execute query and store records in DataFrame: df\n",
    "df = pd.read_sql_query('SELECT * FROM Album', engine)\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Open engine in context manager and store query result in df1\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT * FROM Album\")\n",
    "    df1 = pd.DataFrame(rs.fetchall())\n",
    "    df1.columns = rs.keys()\n",
    "\n",
    "# Confirm that both methods yield the same result\n",
    "print(df.equals(df1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b634eefd",
   "metadata": {},
   "source": [
    "Inner Join ---- Assign to rs the results from the following query: select all the records, extracting the Title of the record and Name of the artist of each record from the Album table and the Artist table, respectively. To do so, INNER JOIN these two tables on the ArtistID column of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a571c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT Title, Name FROM Album INNER JOIN Artist on Album.ArtistID = Artist.ArtistID\")\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "    df.columns = rs.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09987482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute query and store records in DataFrame: df\n",
    "df = pd.read_sql_query('SELECT * FROM PlaylistTrack INNER JOIN Track on PlaylistTrack.TrackId = Track.TrackId WHERE Milliseconds < 250000', engine)\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230d0de3",
   "metadata": {},
   "source": [
    "Intermediate Importing Data\n",
    "url retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b584f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Save file locally\n",
    "urlretrieve(url, 'winequality-red.csv')\n",
    "\n",
    "# Read file into a DataFrame and print its head\n",
    "df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fb277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "urlretrieve(url, 'file.csv')\n",
    "\n",
    "# Read file into a DataFrame: df\n",
    "df = pd.read_csv('file.csv', sep = ';')\n",
    "\n",
    "# Print the head of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd1a96d",
   "metadata": {},
   "source": [
    "Excel sheets and importing -----> using pd.read_excel('file.xls, sheet_name = None)\n",
    "\n",
    "pd.read_excel creates a dict with keys = sheet names and values = dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0516e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import pandas as pd\n",
    "from urllib.request import urlretrieve\n",
    "# Assign url of file: url\n",
    "url = 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'\n",
    "urlretrieve(url, 'file.xls')\n",
    "# Read in all sheets of Excel file: xls\n",
    "dfdict = pd.read_excel('file.xls', sheet_name = None)\n",
    "\n",
    "\n",
    "\n",
    "# Print the sheetnames to the shell\n",
    "print(dfdict.keys())\n",
    "\n",
    "# Print the head of the first sheet (using its name, NOT its index)\n",
    "print(dfdict['1700'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703a8562",
   "metadata": {},
   "source": [
    "Performing HTTP requests using urllib\n",
    "Import the functions urlopen and Request from the subpackage urllib.request.\n",
    "Package the request to the url \"https://campus.datacamp.com/courses/1606/4135?ex=2\" using the function Request() and assign it to request.\n",
    "Send the request and catch the response in the variable response with the function urlopen().\n",
    "Run the rest of the code to see the datatype of response and to close the connection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d348eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the url\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "\n",
    "# This packages the request: request\n",
    "request = Request(url)\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Print the datatype of response\n",
    "print(type(response))\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a9e13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the url\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "\n",
    "# This packages the request\n",
    "request = Request(url)\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Extract the response: html\n",
    "html = response.read()\n",
    "\n",
    "# Print the html\n",
    "print(html)\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f22d6f9",
   "metadata": {},
   "source": [
    "Alternative using requests library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884afbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import requests\n",
    "\n",
    "# Specify the url: url\n",
    "url = \"http://www.datacamp.com/teach/documentation\"\n",
    "\n",
    "# Packages the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response: text\n",
    "text = r.text\n",
    "\n",
    "# Print the html\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f136bd",
   "metadata": {},
   "source": [
    "Simple Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b782f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extracts the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Prettify the BeautifulSoup object: pretty_soup\n",
    "pretty_soup = soup.prettify()\n",
    "\n",
    "# Print the response\n",
    "print(pretty_soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf37d29d",
   "metadata": {},
   "source": [
    "In the sample code, the HTML response object html_doc has already been created: your first task is to Soupify it using the function BeautifulSoup() and to assign the resulting soup to the variable soup.\n",
    "Extract the title from the HTML soup soup using the attribute title and assign the result to guido_title.\n",
    "Print the title of Guido's webpage to the shell using the print() function.\n",
    "Extract the text from the HTML soup soup using the method get_text() and assign to guido_text.\n",
    "Hit submit to print the text from Guido's webpage to the shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad2f7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Get the title of Guido's webpage: guido_title\n",
    "guido_title = soup.title\n",
    "\n",
    "# Print the title of Guido's webpage to the shell\n",
    "print(guido_title)\n",
    "\n",
    "# Get Guido's text: guido_text\n",
    "guido_text = soup.get_text()\n",
    "\n",
    "# Print Guido's text to the shell\n",
    "print(guido_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeade9a",
   "metadata": {},
   "source": [
    "Use the method find_all() to find all hyperlinks in soup, remembering that hyperlinks are defined by the HTML tag <a> but passed to find_all() without angle brackets; store the result in the variable a_tags.\n",
    "The variable a_tags is a results set: your job now is to enumerate over it, using a for loop and to print the actual URLs of the hyperlinks; to do this, for every element link in a_tags, you want to print() link.get('href')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76029972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extracts the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Print the title of Guido's webpage\n",
    "print(soup.title)\n",
    "\n",
    "# Find all 'a' tags (which define hyperlinks): a_tags\n",
    "a_tags = soup.find_all(\"a\")\n",
    "\n",
    "# Print the URLs to the shell\n",
    "for link in a_tags:\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcec5dd",
   "metadata": {},
   "source": [
    "API: Application Programming Interface \n",
    "Protocols and routines - building and interacting with software applications\n",
    "\n",
    "JSON file format - standard form for transferring data through APIs\n",
    "javascript object notation\n",
    "\n",
    "JSON .load will load data as a dict in python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e9d4de",
   "metadata": {},
   "source": [
    "Load the JSON 'a_movie.json' into the variable json_data within the context provided by the with statement. To do so, use the function json.load() within the context manager.\n",
    "Use a for loop to print all key-value pairs in the dictionary json_data. Recall that you can access a value in a dictionary using the syntax: dictionary[key]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ae4b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON: json_data\n",
    "with open(\"a_movie.json\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "# Print each key-value pair in json_data\n",
    "for k in json_data.keys():\n",
    "    print(k + ': ', json_data[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e03491d",
   "metadata": {},
   "source": [
    "APIs are a set of protocols and routines for building an application\n",
    "\n",
    "Another way to think about it is that APIs are code that allows two software programs to communicate with each other\n",
    "\n",
    "eg, using twitter -> use twitter API, \n",
    "program using wikipedia -> use wikipedia API\n",
    "\n",
    "? -----     query strings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76d1712",
   "metadata": {},
   "source": [
    "Import the requests package.\n",
    "Assign to the variable url the URL of interest in order to query 'http://www.omdbapi.com' for the data corresponding to the movie The Social Network. The query string should have two arguments: apikey=72bc447a and t=the+social+network. You can combine them as follows: apikey=72bc447a&t=the+social+network.\n",
    "Print the text of the response object r by using its text attribute and passing the result to the print() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116bd53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import requests package\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = 'http://www.omdbapi.com/?apikey=72bc447a&t=the+social+network'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Print the text of the response\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06a02cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data = r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b05f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign URL to variable: url\n",
    "url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data = r.json()\n",
    "\n",
    "# Print the Wikipedia page extract\n",
    "pizza_extract = json_data['query']['pages']['24768']['extract']\n",
    "print(pizza_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46f6ca8",
   "metadata": {},
   "source": [
    "Twitter APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c935ae",
   "metadata": {},
   "source": [
    "Import the package tweepy.\n",
    "Pass the parameters consumer_key and consumer_secret to the function tweepy.OAuthHandler().\n",
    "Complete the passing of OAuth credentials to the OAuth handler auth by applying to it the method set_access_token(), along with arguments access_token and access_token_secret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0dba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import tweepy\n",
    "\n",
    "# Store OAuth authentication credentials in relevant variables\n",
    "access_token = \"1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy\"\n",
    "access_token_secret = \"X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx\"\n",
    "consumer_key = \"nZ6EA0FxZ293SxGNg8g8aP0HM\"\n",
    "consumer_secret = \"fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i\"\n",
    "\n",
    "# Pass OAuth details to tweepy's OAuth handler\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf30344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Stream listener\n",
    "l = MyStreamListener()\n",
    "\n",
    "# Create your Stream object with authentication\n",
    "stream = tweepy.Stream(auth, l)\n",
    "\n",
    "#To filter Twitter streams, pass to the track argument\n",
    "#in stream.filter() a list containing the desired keywords 'clinton', 'trump', 'sanders', and 'cruz'.\n",
    "# Filter Twitter Streams to capture data by the keywords:\n",
    "stream.filter(['clinton', 'trump', 'sanders', 'cruz'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a5547f",
   "metadata": {},
   "source": [
    "Assign the filename 'tweets.txt' to the variable tweets_data_path.\n",
    "Initialize tweets_data as an empty list to store the tweets in.\n",
    "Within the for loop initiated by for line in tweets_file:, load each tweet into a variable, tweet, using json.loads(), then append tweet to tweets_data using the append() method.\n",
    "Hit submit and check out the keys of the first tweet dictionary printed to the shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2458d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# String of path to file: tweets_data_path\n",
    "tweets_data_path = 'tweets.txt'\n",
    "\n",
    "# Initialize empty list to store tweets: tweets_data\n",
    "tweets_data = []\n",
    "\n",
    "# Open connection to file\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "\n",
    "# Read in tweets and store in list: tweets_data\n",
    "for line in tweets_file:\n",
    "    tweet = json.loads(line)\n",
    "    tweets_data.append(tweet)\n",
    "\n",
    "# Close connection to file\n",
    "tweets_file.close()\n",
    "\n",
    "# Print the keys of the first tweet dict\n",
    "print(tweets_data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ac2e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build DataFrame of tweet texts and languages\n",
    "df = pd.DataFrame(tweets_data, columns=['text', 'lang']) # note text and lang are keys in the dictionaries\n",
    "# first arg tweets_data : list of dictionaries\n",
    "# second argument: list of keys that are the columns of the df\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72185ad1",
   "metadata": {},
   "source": [
    "Interesting python note: variable eg, myVar = 0\n",
    "\n",
    "myVar += True\n",
    "-> myVar = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e2a709",
   "metadata": {},
   "source": [
    "Plotting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of labels:cd\n",
    "cd = ['clinton', 'trump', 'sanders', 'cruz']\n",
    "\n",
    "# Plot the bar chart\n",
    "ax = sns.barplot(x = cd, y = [clinton, trump, sanders, cruz])\n",
    "ax.set(ylabel=\"count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8130ac8",
   "metadata": {},
   "source": [
    "pandas has a dtype 'category'\n",
    "The categorical data type is useful in the following cases:\n",
    "\n",
    "A string variable consisting of only a few different values. Converting such a string variable to a categorical variable will save some memory, see here.\n",
    "\n",
    "The lexical order of a variable is not the same as the logical order (“one”, “two”, “three”). By converting to a categorical and specifying an order on the categories, sorting and min/max will use the logical order instead of the lexical order, see here.\n",
    "\n",
    "As a signal to other Python libraries that this column should be treated as a categorical variable (e.g. to use suitable statistical methods or plot types)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9c9843",
   "metadata": {},
   "source": [
    "Use the .strip() method to strip duration of \"minutes\" and store it in the duration_trim column.\n",
    "Convert duration_trim to int and store it in the duration_time column.\n",
    "Write an assert statement that checks if duration_time's data type is now an int.\n",
    "Print the average ride duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f909a87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip duration of minutes\n",
    "ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip(\"minutes\")\n",
    "\n",
    "# Convert duration to integer\n",
    "ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype('int')\n",
    "\n",
    "# Write an assert statement making sure of conversion\n",
    "assert ride_sharing['duration_time'].dtype == 'int'\n",
    "\n",
    "# Print formed columns and calculate average ride duration \n",
    "print(ride_sharing[['duration','duration_trim','duration_time']])\n",
    "print(ride_sharing['duration_time'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be24cc",
   "metadata": {},
   "source": [
    "Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2498c56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tire_sizes to integer\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n",
    "\n",
    "# Set all values above 27 to 27\n",
    "ride_sharing.loc[ride_sharing['tire_sizes'] > 27, 'tire_sizes'] = 27\n",
    "\n",
    "# Reconvert tire_sizes back to categorical\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')\n",
    "\n",
    "# Print tire size description\n",
    "print(ride_sharing['tire_sizes'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8a2663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ride_date to datetime\n",
    "ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date'])\n",
    "\n",
    "# Save today's date\n",
    "today = dt.date.today()\n",
    "\n",
    "# Set all in the future to today's date\n",
    "ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today\n",
    "\n",
    "# Print maximum of ride_dt column\n",
    "print(ride_sharing['ride_dt'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30840858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicates\n",
    "duplicates = ride_sharing.duplicated(subset = 'ride_id', keep = False)\n",
    "\n",
    "# Sort your duplicated rides\n",
    "duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')\n",
    "\n",
    "# Print relevant columns\n",
    "print(duplicated_rides[['ride_id','duration','user_birth_year']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4808ddd9",
   "metadata": {},
   "source": [
    "Drop complete duplicates in ride_sharing and store the results in ride_dup.\n",
    "Create the statistics dictionary which holds minimum aggregation for user_birth_year and mean aggregation for duration.\n",
    "Drop incomplete duplicates by grouping by ride_id and applying the aggregation in statistics.\n",
    "Find duplicates again and run the assert statement to verify de-duplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faec3047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop complete duplicates from ride_sharing\n",
    "ride_dup = ride_sharing.drop_duplicates()\n",
    "\n",
    "# Create statistics dictionary for aggregation function\n",
    "statistics = {'user_birth_year': 'min', 'duration': 'mean'}\n",
    "\n",
    "# Group by ride_id and compute new statistics\n",
    "ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()\n",
    "\n",
    "# Find duplicated values again\n",
    "duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)\n",
    "duplicated_rides = ride_unique[duplicates == True]\n",
    "\n",
    "# Assert duplicates are processed\n",
    "assert duplicated_rides.shape[0] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb0529b",
   "metadata": {},
   "source": [
    "Consistent and inconsistent\n",
    "- categories that are inside both dataframes or only one in or the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d57e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "inconsistent_categories = set(study_data['blood_type']).difference(categories['blood_type'])\n",
    "inconsistent_rows = study_data['blood_type'].isin(inconsistent_categories)\n",
    "inconsistent_data = study_data[inconsistent_rows]\n",
    "# drop inconsistent categories and get consistent data only\n",
    "consistent_data = study_data[~inconsistent_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26f3e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the cleanliness category in airlines not in categories\n",
    "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "\n",
    "# Find rows with that category\n",
    "cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
    "\n",
    "# Print rows with inconsistent category\n",
    "print(airlines[cat_clean_rows])\n",
    "\n",
    "# Print rows with consistent categories only\n",
    "print(airlines[~cat_clean_rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a7c012",
   "metadata": {},
   "source": [
    "Mapping Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efc9cde",
   "metadata": {},
   "source": [
    "Create the ranges and labels for the wait_type column mentioned in the description.\n",
    "Create the wait_type column by from wait_min by using pd.cut(), while inputting label_ranges and label_names in the correct arguments.\n",
    "Create the mapping dictionary mapping weekdays to 'weekday' and weekend days to 'weekend'.\n",
    "Create the day_week column by using .replace()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9415a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ranges for categories\n",
    "label_ranges = [0, 60, 180, np.inf]\n",
    "label_names = ['short', 'medium', 'long']\n",
    "\n",
    "# Create wait_type column\n",
    "airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, \n",
    "                                labels = label_names)\n",
    "\n",
    "# Create mappings and replace\n",
    "mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', \n",
    "            'Thursday': 'weekday', 'Friday': 'weekday', \n",
    "            'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
    "\n",
    "airlines['day_week'] = airlines['day'].replace(mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75cc595",
   "metadata": {},
   "source": [
    "np.nan creates NaN object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51342d2b",
   "metadata": {},
   "source": [
    "Using the airlines DataFrame, store the length of each instance in the survey_response column in resp_length by using .str.len().\n",
    "Isolate the rows of airlines with resp_length higher than 40.\n",
    "Assert that the smallest survey_response length in airlines_survey is now bigger than 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2510d74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store length of each row in survey_response column\n",
    "resp_length = airlines['survey_response'].str.len()\n",
    "\n",
    "# Find rows in airlines where resp_length > 40\n",
    "airlines_survey = airlines[resp_length > 40]\n",
    "\n",
    "# Assert minimum survey_response length is > 40\n",
    "assert airlines_survey['survey_response'].str.len().min() > 40\n",
    "\n",
    "# Print new survey_response column\n",
    "print(airlines_survey['survey_response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7d49f5",
   "metadata": {},
   "source": [
    "Unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b15b0a",
   "metadata": {},
   "source": [
    "Find the rows of acct_cur in banking that are equal to 'euro' and store them in the variable acct_eu.\n",
    "Find all the rows of acct_amount in banking that fit the acct_eu condition, and convert them to USD by multiplying them with 1.1.\n",
    "Find all the rows of acct_cur in banking that fit the acct_eu condition, set them to 'dollar'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab8404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find values of acct_cur that are equal to 'euro'\n",
    "acct_eu = banking['acct_cur'] == 'euro'\n",
    "\n",
    "# Convert acct_amount where it is in euro to dollars\n",
    "banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1\n",
    "\n",
    "# Unify acct_cur column by changing 'euro' values to 'dollar'\n",
    "banking.loc[acct_eu, 'acct_cur'] = 'dollar'\n",
    "\n",
    "# Assert that only dollar currency remains\n",
    "assert banking['acct_cur'].unique() == 'dollar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ef7122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the header of account_opend\n",
    "print(banking['account_opened'].head())\n",
    "\n",
    "# Convert account_opened to datetime\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
    "                                           # Infer datetime format\n",
    "                                           infer_datetime_format = True,\n",
    "                                           # Return missing value for error\n",
    "                                           errors = 'coerce') \n",
    "\n",
    "# Get year of account opened\n",
    "banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')\n",
    "\n",
    "# Print acct_year\n",
    "print(banking['acct_year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3817a952",
   "metadata": {},
   "source": [
    "Crossfield validation - not the same as data cleaning (out of range) nor datatype checking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3ecfe9",
   "metadata": {},
   "source": [
    "Row wise summing is done with .sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3af992d",
   "metadata": {},
   "source": [
    "Find the rows where the sum of all rows of the fund_columns in banking are equal to the inv_amount column.\n",
    "Store the values of banking with consistent inv_amount in consistent_inv, and those with inconsistent ones in inconsistent_inv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fa6388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store fund columns to sum against\n",
    "fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n",
    "\n",
    "# Find rows where fund_columns row sum == inv_amount\n",
    "inv_equ = banking[fund_columns].sum(axis = 1) == banking['inv_amount']\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_inv = banking[inv_equ]\n",
    "inconsistent_inv = banking[~inv_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eb8434",
   "metadata": {},
   "source": [
    "Find the rows where the sum of all rows of the fund_columns in banking are equal to the inv_amount column.\n",
    "Store the values of banking with consistent inv_amount in consistent_inv, and those with inconsistent ones in inconsistent_inv.\n",
    "Store today's date into today, and manually calculate customers' ages and store them in ages_manual.\n",
    "Find all rows of banking where the age column is equal to ages_manual and then filter banking into consistent_ages and inconsistent_ages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5c10a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store today's date and find ages\n",
    "today = dt.date.today()\n",
    "ages_manual = today.year - banking['birth_date'].dt.year\n",
    "\n",
    "# Find rows where age column == ages_manual\n",
    "age_equ = banking['age'] == ages_manual\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_ages = banking[age_equ]\n",
    "inconsistent_ages = banking[~age_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent ages: \", inconsistent_ages.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbef48d",
   "metadata": {},
   "source": [
    "Completeness (missing data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0540070a",
   "metadata": {},
   "source": [
    "try out missingno library\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38318027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of missing values in banking\n",
    "print(banking.isna().sum())\n",
    "\n",
    "# Visualize missingness matrix\n",
    "msno.matrix(banking)\n",
    "plt.show()\n",
    "\n",
    "# Isolate missing and non missing values of inv_amount\n",
    "missing_investors = banking[banking['inv_amount'].isna()]\n",
    "investors = banking[~banking['inv_amount'].isna()]\n",
    "\n",
    "# Sort banking by age and visualize\n",
    "banking_sorted = banking.sort_values('age')\n",
    "msno.matrix(banking_sorted)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1868e8",
   "metadata": {},
   "source": [
    "Use .dropna() to drop missing values of the cust_id column in banking and store the results in banking_fullid.\n",
    "Use inv_amount to compute the estimated account amounts for banking_fullid by setting the amounts equal to inv_amount * 5, and assign the results to acct_imp.\n",
    "Impute the missing values of acct_amount in banking_fullid with the newly created acct_imp using .fillna()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dd219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values of cust_id\n",
    "banking_fullid = banking.dropna(subset = ['cust_id'])\n",
    "\n",
    "# Compute estimated acct_amount\n",
    "acct_imp = banking_fullid['inv_amount'] * 5\n",
    "\n",
    "# Impute missing acct_amount with corresponding acct_imp\n",
    "banking_imputed = banking_fullid.fillna({'acct_amount':acct_imp})\n",
    "\n",
    "# Print number of missing values\n",
    "print(banking_imputed.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1b3de1",
   "metadata": {},
   "source": [
    "Record linkage\n",
    "\n",
    "fuzzywuzzy - using levenshtein minimum edit distance algorithm -> insertion, substituion, deletion\n",
    "\n",
    "fuzzywuzzy fuzz -> compares strings and returns a number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc42690f",
   "metadata": {},
   "source": [
    " Now you can iterate through matches to reassign similar entries.\n",
    "\n",
    "Within the for loop, use an if statement to check whether the similarity score in each match is greater than or equal to 80.\n",
    "If it is, use .loc to select rows where cuisine_type in restaurants is equal to the current match (which is the first element of match), and reassign them to be 'italian'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ce6d96",
   "metadata": {},
   "source": [
    "You can use .loc[] combined with a condition to reassign the matches that are similar to 'italian' to 'italian' using the following syntax: df.loc[df['column'] == old_value] = new_value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216aea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of matches, comparing 'italian' with the cuisine_type column\n",
    "matches = process.extract('italian', restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
    "\n",
    "# Iterate through the list of matches to italian\n",
    "for match in matches:\n",
    "  # Check whether the similarity score is greater than or equal to 80\n",
    "  if match[1] >= 80:\n",
    "    # Select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "    restaurants.loc[restaurants['cuisine_type'] == match[0]] = 'italian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441e309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through categories\n",
    "for cuisine in categories:  \n",
    "  # Create a list of matches, comparing cuisine with the cuisine_type column\n",
    "  matches = process.extract(cuisine, restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
    "\n",
    "  # Iterate through the list of matches\n",
    "  for match in matches:\n",
    "     # Check whether the similarity score is greater than or equal to 80\n",
    "    if match[1] >= 80:\n",
    "      # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "      restaurants.loc[restaurants['cuisine_type'] == match[0]] = cuisine\n",
    "      \n",
    "# Inspect the final result\n",
    "print(restaurants['cuisine_type'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2159b73c",
   "metadata": {},
   "source": [
    "Record linkage - more complex than traditional join -> ie. no unique identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afda6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using recordlinkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f123182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an indexer and object and find possible pairs\n",
    "indexer = recordlinkage.Index()\n",
    "\n",
    "# Block pairing on cuisine_type\n",
    "indexer.block('cuisine_type')\n",
    "\n",
    "# Generate pairs\n",
    "pairs = indexer.index(restaurants, restaurants_new)\n",
    "\n",
    "# Create a comparison object\n",
    "comp_cl = recordlinkage.Compare()\n",
    "\n",
    "# Find exact matches on city, cuisine_types - \n",
    "comp_cl.exact('city', 'city', label='city')\n",
    "comp_cl.exact('cuisine_type', 'cuisine_type', label='cuisine_type')\n",
    "\n",
    "# Find similar matches of rest_name\n",
    "comp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.8) \n",
    "\n",
    "# Get potential matches and print\n",
    "potential_matches = comp_cl.compute(pairs, restaurants, restaurants_new)\n",
    "print(potential_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2786da7",
   "metadata": {},
   "source": [
    "Isolate instances of potential_matches where the row sum is above or equal to 3 by using the .sum() method.\n",
    "Extract the second column index from matches, which represents row indices of matching record from restaurants_new by using the .get_level_values() method.\n",
    "Subset restaurants_new for rows that are not in matching_indices.\n",
    "Append non_dup to restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991e5f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate potential matches with row sum >=3\n",
    "matches = potential_matches[potential_matches.sum(axis = 1) >= 3]\n",
    "#potential matches is a dataframe created earlier\n",
    "\n",
    "# Get values of second column index of matches\n",
    "matching_indices = matches.index.get_level_values(1)\n",
    "\n",
    "# Subset restaurants_new based on non-duplicate values\n",
    "non_dup = restaurants_new[~restaurants_new.index.isin(matching_indices)]\n",
    "\n",
    "# Append non_dup to restaurants\n",
    "full_restaurants = restaurants.append(non_dup)\n",
    "print(full_restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ca510",
   "metadata": {},
   "source": [
    "Notes from assessment: - import data from a csv, using only 3 columns\n",
    "\n",
    "wine = pd.read_csv(file_name, usecols = cols)\n",
    "\n",
    "Change a date time object into another % % % format:\n",
    "date_of_birth = datetime.date(1986, 12, 9)\n",
    "fmt = '%b %d %Y'\n",
    "date_of_birth.strftime(fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5c7040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b27d44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885908b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3bd113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ab91de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7549a228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae58d31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8efe71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e2a29a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30510755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6554292b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3c9363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35860eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1897ad9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4855ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022fdaad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c113b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d343ddd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ad070f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e36e137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3602d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8559a926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5369bd5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859f8581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bd334e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b65cd9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e92b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184093ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c17fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae2c1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44b4796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2896f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873a111d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc288b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c90eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffadd871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a5d466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca6be0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1658410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c445e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b06051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
